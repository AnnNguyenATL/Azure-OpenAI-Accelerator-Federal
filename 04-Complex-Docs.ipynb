{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60ec6048-44e4-4118-b16a-9c4c9cc78a3b",
      "metadata": {},
      "source": [
        "# How to deal with complex/large Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9281ac79-47cd-49d4-bdd4-7f5c173a947d",
      "metadata": {},
      "source": [
        "In the previous notebook, we developed a solution for various types of files and data formats commonly found in organizations, and this covers 90% of the use cases. However, you will find that there are issues when dealing with questions that require answers from complex files. The complexity of these files arises from their length and the way information is distributed within them. Large documents are always a challenge for Search Engines.\n",
        "\n",
        "One example of such complex files is Technical Specification Guides or Product Manuals, which can span hundreds of pages and contain information in the form of images, tables, forms, and more. Books are also complex due to their length and the presence of images or tables.\n",
        "\n",
        "These files are typically in PDF format. To better handle these PDFs, we need a smarter parsing method that treats each document as a special source and processes them page by page. The objective is to obtain more accurate and faster answers from our system. Fortunately, there are usually not many of these types of documents in an organization, allowing us to make exceptions and treat them differently.\n",
        "\n",
        "If your use case is just PDFs, for example, you can just use [PyPDF library](https://pypi.org/project/pypdf/) or [Azure AI Document Intelligence SDK (former Form Recognizer)](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-3.0.0), vectorize using OpenAI API and push the content to a vector-based index. And this is problably the simplest and fastest way to go.  However if your use case entails connecting to a datalake, or Sharepoint libraries or any other document data source with thousands of documents with multiple file types and that can change dynamically, then you would want to use the Ingestion and Document Cracking and AI-Enrichment capabilities of Azure Search engine, Notebooks 1-3, and avoid a lot of painful custom code. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import langchain\n",
        "from langchain.embeddings import AzureOpenAIEmbeddings\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "\n",
        "from common.utils import parse_pdf, read_pdf_files, text_to_base64\n",
        "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_PROMPT_TEMPLATE\n",
        "from common.utils import (\n",
        "    get_search_results,\n",
        "    model_tokens_limit,\n",
        "    num_tokens_from_docs,\n",
        "    num_tokens_from_string\n",
        ")\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\", override=True)\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "    \n",
        "os.makedirs(\"data/books/\",exist_ok=True)\n",
        "    \n",
        "\n",
        "BLOB_CONTAINER_NAME = \"books\"\n",
        "BASE_CONTAINER_URL = \"https://datasetsgptsmartsearch.blob.core.windows.net/\" + BLOB_CONTAINER_NAME + \"/\"\n",
        "LOCAL_FOLDER = \"./data/books/\"\n",
        "\n",
        "MODEL = \"gpt-4-32k\" # options: gpt-35-turbo, gpt-35-turbo-16k, gpt-4, gpt-4-32k\n",
        "\n",
        "os.makedirs(LOCAL_FOLDER,exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "331692ba-b68e-4b99-9bae-5057da9a389d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
      "metadata": {},
      "outputs": [],
      "source": [
        "embedder = AzureOpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb87c647-158c-4f85-b569-5b9462f06c83",
      "metadata": {},
      "source": [
        "## 1 - Manual Document Cracking with Push to Vector-based Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75551868-1546-421b-a14e-e42618d88e61",
      "metadata": {},
      "source": [
        "Within our demo storage account, we have a container named `books`, which holds 5 books of different lengths, languages, and complexities. Let's create a `cogsrch-index-books-vector` and load it with the pages of all these books.\n",
        "\n",
        "We begin by downloading these books to our local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0999e24b-6a75-4fa1-9a5f-426cf0f0bdba",
      "metadata": {},
      "outputs": [],
      "source": [
        "books = [\"Azure_Cognitive_Search_Documentation.pdf\", \n",
        "         \"Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf\",\n",
        "         \"Fundamentals_of_Physics_Textbook.pdf\",\n",
        "         \"Made_To_Stick.pdf\",\n",
        "         \"Pere_Riche_Pere_Pauvre.pdf\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd867b2f-b5a1-443c-aa0a-ce914a66b3c9",
      "metadata": {},
      "source": [
        "Let's download the files to the local `./data/` folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3554f0b7-fee8-4446-a155-5d22dc0f0888",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:20<00:00,  4.04s/it]\n"
          ]
        }
      ],
      "source": [
        "for book in tqdm(books):\n",
        "    book_url = BASE_CONTAINER_URL + book + os.environ['BLOB_SAS_TOKEN']\n",
        "    urllib.request.urlretrieve(book_url, LOCAL_FOLDER+ book)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788cc0db-9dae-45f2-8943-2b6fa32fcc75",
      "metadata": {},
      "source": [
        "### What to use: pyPDF or AI Documment Intelligence API (Form Recognizer)?\n",
        "\n",
        "In `utils.py` there is a **parse_pdf()** function. This utility function can parse local files using PyPDF library and can also parse local or from_url PDFs files using Azure AI Document Intelligence (Former Form Recognizer).\n",
        "\n",
        "If `form_recognizer=False`, the function will parse the PDF using the python pyPDF library, which 75% of the time does a good job.<br>\n",
        "\n",
        "Setting `form_recognizer=True`, is the best (and slower) parsing method using AI Documment Intelligence API (former known as Form Recognizer). You can specify the prebuilt model to use, the default is `model=\"prebuilt-document\"`. However, if you have a complex document with tables, charts and figures , you can try\n",
        "`model=\"prebuilt-layout\"`, and it will capture all of the nuances of each page (it takes longer of course).\n",
        "\n",
        "**Note: Many PDFs are scanned images. For example, any signed contract that was scanned and saved as PDF will NOT be parsed by pyPDF. Only AI Documment Intelligence API will work.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c1c63a2f-7a53-4346-8a1f-483cfd159d34",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting Text from Azure_Cognitive_Search_Documentation.pdf ...\n",
            "Extracting text using PyPDF\n",
            "Parsing took: 32.443884 seconds\n",
            "Azure_Cognitive_Search_Documentation.pdf contained 1947 pages\n",
            "\n",
            "Extracting Text from Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf ...\n",
            "Extracting text using PyPDF\n",
            "Parsing took: 1.853975 seconds\n",
            "Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf contained 357 pages\n",
            "\n",
            "Extracting Text from Fundamentals_of_Physics_Textbook.pdf ...\n",
            "Extracting text using PyPDF\n",
            "Parsing took: 93.533881 seconds\n",
            "Fundamentals_of_Physics_Textbook.pdf contained 1450 pages\n",
            "\n",
            "Extracting Text from Made_To_Stick.pdf ...\n",
            "Extracting text using PyPDF\n",
            "Parsing took: 6.783200 seconds\n",
            "Made_To_Stick.pdf contained 225 pages\n",
            "\n",
            "Extracting Text from Pere_Riche_Pere_Pauvre.pdf ...\n",
            "Extracting text using PyPDF\n",
            "Parsing took: 0.691667 seconds\n",
            "Pere_Riche_Pere_Pauvre.pdf contained 225 pages\n",
            "\n"
          ]
        }
      ],
      "source": [
        "book_pages_map = dict()\n",
        "for book in books:\n",
        "    print(\"Extracting Text from\",book,\"...\")\n",
        "    \n",
        "    # Capture the start time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Parse the PDF\n",
        "    book_path = LOCAL_FOLDER+book\n",
        "    book_map = parse_pdf(file=book_path, form_recognizer=False, verbose=True)\n",
        "    book_pages_map[book]= book_map\n",
        "    \n",
        "    # Capture the end time and Calculate the elapsed time\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(f\"Parsing took: {elapsed_time:.6f} seconds\")\n",
        "    print(f\"{book} contained {len(book_map)} pages\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5de0a722-ae0c-4b57-802a-518f5d4d93fd",
      "metadata": {},
      "source": [
        "Now let's check a random page of each book to make sure the parsing was done correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f2a5d62f-b664-4662-a6c9-a1eb2a3c5e11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure_Cognitive_Search_Documentation.pdf \n",
            " chunk text: Stack Overflow: Azure Cognitive Search  \n",
            "How full text search works in Azure Cog ...\n",
            "\n",
            "Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf \n",
            " chunk text: 42\n",
            "from people who act in destructive ways (Matt. 18:15–17; 1 Cor.\n",
            "5:9–13). We a ...\n",
            "\n",
            "Fundamentals_of_Physics_Textbook.pdf \n",
            " chunk text: 1CHAPTER 1Measurement1-1MEASURING THINGS, INCLUDING LENGTHSLearning ObjectivesAf ...\n",
            "\n",
            "Made_To_Stick.pdf \n",
            " chunk text: inforce the need for compactness and to provide a h int about how to \n",
            "cram more  ...\n",
            "\n",
            "Pere_Riche_Pere_Pauvre.pdf \n",
            " chunk text: ~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for bookname,bookmap in book_pages_map.items():\n",
        "    print(bookname,\"\\n\",\"chunk text:\",bookmap[random.randint(10, 50)][2][:80],\"...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcdc1ee-71fc-49d2-8e7c-0964bc3a4370",
      "metadata": {},
      "source": [
        "As we can see above, all books were parsed except `Pere_Riche_Pere_Pauvre.pdf` (this book is \"Rich Dad, Poor Dad\" written in French), why? Well, as we mentioned above, this book was scanned, so each page is an image and with a very unique font. We need a good PDF parser with good OCR capabilities in order to extract the content of this PDF. \n",
        "Let's try to parse this book again, but this time using Azure Document Intelligence API (former Form Recognizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "801c6bc2-467c-4418-aa7e-ef89a1e20e1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting text using Azure Document Intelligence\n",
            "CPU times: total: 5.55 s\n",
            "Wall time: 1min 14s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "book = \"Pere_Riche_Pere_Pauvre.pdf\"\n",
        "book_path = LOCAL_FOLDER+book\n",
        "book_map = parse_pdf(file=book_path, form_recognizer=True, model=\"prebuilt-document\",from_url=False, verbose=True)\n",
        "book_pages_map[book]= book_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "629476fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Note: If the above command throws an error - Create another form recognizer resource in the azure portal in the same resource group, don't delete it. And try again.\n",
        "# This seems to be a transient error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "97f9c5bb-c44b-4a4d-9780-591f9f8d128a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pere_Riche_Pere_Pauvre.pdf \n",
            " chunk text: « Vous avez dit que vous m'enseigneriez si je travaillais pour vous. Eh bien, j' ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(book,\"\\n\",\"chunk text:\",book_map[random.randint(10, 50)][2][:80],\"...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c279dfb-4fed-41b8-89e1-0ca2cefbcdc9",
      "metadata": {},
      "source": [
        "As demonstrated above, Azure Document Intelligence proves to be superior to pyPDF. **For production scenarios, we strongly recommend using Azure Document Intelligence consistently**. When doing so, it's important to make a wise choice between the available models, such as \"prebuilt-document,\" \"prebuilt-layout,\" or others. You can find more information on model selection [HERE](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/choose-model-feature?view=doc-intel-3.0.0).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
      "metadata": {},
      "source": [
        "## Create Vector-based index\n",
        "\n",
        "\n",
        "Now that we have the content of the book's chunks (each page of each book) in the dictionary `book_pages_map`, let's create the Vector-based index in our Azure Search Engine where this content is going to land"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "book_index_name = \"cogsrch-index-books-vector\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create Azure Search Vector-based Index\n",
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "204\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "index_payload = {\n",
        "    \"name\": book_index_name,\n",
        "    \"fields\": [\n",
        "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
        "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
        "        {\"name\": \"chunk\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
        "        {\"name\": \"chunkVector\",\"type\": \"Collection(Edm.Single)\",\"searchable\": \"true\",\"retrievable\": \"true\",\"dimensions\": 1536,\"vectorSearchConfiguration\": \"vectorConfig\"},\n",
        "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
        "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
        "        {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
        "        \n",
        "    ],\n",
        "    \"vectorSearch\": {\n",
        "        \"algorithmConfigurations\": [\n",
        "            {\n",
        "                \"name\": \"vectorConfig\",\n",
        "                \"kind\": \"hnsw\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"semantic\": {\n",
        "        \"configurations\": [\n",
        "            {\n",
        "                \"name\": \"my-semantic-config\",\n",
        "                \"prioritizedFields\": {\n",
        "                    \"titleField\": {\n",
        "                        \"fieldName\": \"title\"\n",
        "                    },\n",
        "                    \"prioritizedContentFields\": [\n",
        "                        {\n",
        "                            \"fieldName\": \"chunk\"\n",
        "                        }\n",
        "                    ],\n",
        "                    \"prioritizedKeywordsFields\": []\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name,\n",
        "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "36691ff0-c4c8-49d0-bfa8-3e076ece0ce5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to debug errors\n",
        "# r.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc7dda9-4725-410e-9465-54f0298fc758",
      "metadata": {},
      "source": [
        "## Upload the Document chunks and its vectors to the Vector-Based Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d73e7600-7902-48d4-b199-9d9dc0a17aa0",
      "metadata": {},
      "source": [
        "The following code will iterate over each chunk of each book and use the Azure Search Rest API upload method to insert each document with its corresponding vector (using OpenAI embedding model) to the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f5c8aa55-1b60-4057-93db-0d4a89993a57",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading chunks from Azure_Cognitive_Search_Documentation.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1947/1947 [15:54<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading chunks from Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [02:55<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading chunks from Fundamentals_of_Physics_Textbook.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1450/1450 [13:59<00:00,  1.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading chunks from Made_To_Stick.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 225/225 [01:49<00:00,  2.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading chunks from Pere_Riche_Pere_Pauvre.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 225/225 [01:51<00:00,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 10min 6s\n",
            "Wall time: 36min 30s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for bookname,bookmap in book_pages_map.items():\n",
        "    print(\"Uploading chunks from\",bookname)\n",
        "    for page in tqdm(bookmap):\n",
        "        try:\n",
        "            page_num = page[0] + 1\n",
        "            content = page[2]\n",
        "            book_url = BASE_CONTAINER_URL + bookname\n",
        "            upload_payload = {\n",
        "                \"value\": [\n",
        "                    {\n",
        "                        \"id\": text_to_base64(bookname + str(page_num)),\n",
        "                        \"title\": f\"{bookname}_page_{str(page_num)}\",\n",
        "                        \"chunk\": content,\n",
        "                        \"chunkVector\": embedder.embed_query(content if content!=\"\" else \"-------\"),\n",
        "                        \"name\": bookname,\n",
        "                        \"location\": book_url,\n",
        "                        \"page_num\": page_num,\n",
        "                        \"@search.action\": \"upload\"\n",
        "                    },\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name + \"/docs/index\",\n",
        "                                 data=json.dumps(upload_payload), headers=headers, params=params)\n",
        "            if r.status_code != 200:\n",
        "                print(r.status_code)\n",
        "                print(r.text)\n",
        "        except Exception as e:\n",
        "            print(\"Exception:\",e)\n",
        "            print(content)\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715cddcf-af7b-4006-a047-853fc7a66be3",
      "metadata": {},
      "source": [
        "## Query the Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "8b408798-5527-44ca-9dba-cad2ee726aca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# QUESTION = \"what normally rich dad do that is different from poor dad?\"\n",
        "# QUESTION = \"Tell me a summary of the book Boundaries\"\n",
        "# QUESTION = \"Dime que significa la radiacion del cuerpo negro\"\n",
        "# QUESTION = \"what is the acronym of the main point of Made to Stick book\"\n",
        "QUESTION = \"Tell me a python example of how do I push documents with vectors to an index using the python SDK?\"\n",
        "# QUESTION = \"who won the soccer worldcup in 1994?\" # this question should have no answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1b182ade-0ddd-47a1-b1eb-2cbf435c317f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'@odata.context': \"https://gptkb-dyuinno.search.windows.net/indexes('cogsrch-index-books-vector')/$metadata#docs(*)\", '@odata.count': 4081, '@search.answers': [], 'value': [{'@search.score': 0.03159204125404358, '@search.rerankerScore': 1.7060354948043823, '@search.captions': [{'text': '1. The following step executes an empty search ( search=*), returning an unranked list (search score = 1.0) of arbitrary documents. Because there are no criteria, all documents are included in results. This query prints just two of the fields in each document. It also adds include_total_count=True to get a count of all documents (4) in the results.', 'highlights': '1. The following step executes an empty search ( search=*), returning an unranked list (search score = 1.0) of arbitrary<em> documents.</em> Because there are no criteria, all<em> documents are</em> included in results. This query prints just two of the fields in each<em> document.</em> It also adds include_total_count=True to get a count of all<em> documents</em> (4) in the results.'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjE0OA==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_148', 'chunk': 'in the previous step into the Cognitive Search index.\\nPython\\n3. Run each step to push the documents to an index in your search service.\\nThis step shows you how to query an index using the search method of the search.client\\nclass .\\n1. The following step executes an empty search ( search=*), returning an unranked list\\n(search score = 1.0) of arbitrary documents. Because there are no criteria, all\\ndocuments are included in results. This query prints just two of the fields in each\\ndocument. It also adds include_total_count=True to get a count of all documents\\n(4) in the results.\\nPython\\n2. The next query adds whole terms to the search expression (\"wifi\"). This query\\nspecifies that results contain only those fields in the select statement. Limiting the\\nfields that come back minimizes the amount of data sent back over the wire and\\nreduces search latency.\\nPythontry: \\n    result = search_client.upload_documents(documents=documents)  \\n    print( \"Upload of new document succeeded:  \\n{}\".format(result[ 0].succeeded))  \\nexcept Exception as ex:\\n    print (ex.message)  \\n3 - Search an index\\nresults = search_client.search(search_text= \"*\", \\ninclude_total_count= True) \\nprint (\\'Total Documents Matching Query:\\' , results.get_count())  \\nfor result in results:  \\n    print( \"{}: {}\" .format(result[ \"HotelId\" ], result[ \"HotelName\" ])) \\nresults = search_client.search(search_text= \"wifi\", \\ninclude_total_count= True, select= \\'HotelId,HotelName,Tags\\' ) \\nprint (\\'Total Documents Matching Query:\\' , results.get_count())  \\nfor result in results:  ', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.02878725901246071, '@search.rerankerScore': 1.6769300699234009, '@search.captions': [{'text': 'Python 3. Run each step. To load documents, create a documents collection, using an index action  for the operation type (upload, merge-and-upload, and so forth). Documents originate from HotelsData  on GitHub. 1. In a new cell, provide four documents that conform to the index schema. Specify an upload action for each document.', 'highlights': '<em>Python</em> 3. Run each step. To load<em> documents,</em> create a<em> documents</em> collection,<em> using</em> an<em> index</em> action  for the operation type (upload, merge-and-upload, and so forth).<em> Documents</em> originate from HotelsData  on GitHub. 1. In a new cell, provide four<em> documents</em> that conform to the<em> index</em> schema. Specify an upload action for each<em> document.</em>'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjE0NQ==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_145', 'chunk': '2. In another cell, formulate the request. This create_index request targets the indexes\\ncollection of your search service and creates a SearchIndex  based on the index\\nschema you provided in the previous cell.\\nPython\\n3. Run each step.\\nTo load documents, create a documents collection, using an index action  for the\\noperation type (upload, merge-and-upload, and so forth). Documents originate from\\nHotelsData  on GitHub.\\n1. In a new cell, provide four documents that conform to the index schema. Specify\\nan upload action for each document.\\nPython            SearchableField(name= \"Country\" , \\ntype=SearchFieldDataType.String, facetable= True, filterable= True, \\nsortable= True), \\n        ])  \\n    ] \\ncors_options = CorsOptions(allowed_origins=[ \"*\"], \\nmax_age_in_seconds= 60) \\nscoring_profiles = []  \\nsuggester = [{ \\'name\\': \\'sg\\', \\'source_fields\\' : [\\'Tags\\', \\'Address/City\\' , \\n\\'Address/Country\\' ]}] \\nindex = SearchIndex(  \\n    name=name,  \\n    fields=fields,  \\n    scoring_profiles=scoring_profiles,  \\n    suggesters = suggester,  \\n    cors_options=cors_options)  \\ntry: \\n    result = admin_client.create_index(index)  \\n    print (\\'Index\\', result.name, \\'created\\' ) \\nexcept Exception as ex:\\n    print (ex) \\n2 - Load documents\\ndocuments = [  \\n    { \\n    \"@search.action\" : \"upload\" , \\n    \"HotelId\" : \"1\", \\n    \"HotelName\" : \"Secret Point Motel\" , ', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.010101010091602802, '@search.rerankerScore': 1.461320400238037, '@search.captions': [{'text': 'Uplo ad cont ent using the \"push\" model  to push JSON documents from any source, or use the \"pull\" model (indexers)  if your source data is of a supported type. 3.', 'highlights': 'Uplo ad cont ent<em> using</em> the \"push\" model  to<em> push JSON documents</em> from any source, or<em> use</em> the <em>\"pull\" model (indexers)</em>  if your source data is of a supported type. 3.'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjY=', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_6', 'chunk': 'raw content, such as filtering out diacritics, or recognizing and preserving patterns\\nin strings.\\nFor more information about specific functionality, see Features of Azure Cognitive\\nSearch\\nFunctionality is exposed through the Azure portal, simple REST APIs , or Azure SDKs like\\nthe Azure SDK for .NET . The Azure portal supports service administration and content\\nmanagement, with tools for prototyping and querying your indexes and skillsets.\\nAn end-to-end exploration of core search features can be accomplished in four steps:\\n1. Decide on a tier  and region. One free search service is allowed per subscription. All\\nquickstarts can be completed on the free tier. For more capacity and capabilities,\\nyou\\'ll need a billable tier .\\n2. Create a sear ch ser vice in the Azure portal.\\n3. Start with Impor t data wizar d. Choose a built-in sample or a supported data\\nsource to create, load, and query an index in minutes.\\n4. Finish with Sear ch Explor er, using a portal client to query the search index you just\\ncreated.\\nAlternatively, you can create, load, and query a search index in atomic steps:\\n1. Create a sear ch index  using the portal, REST API , .NET SDK , or another SDK. The\\nindex schema defines the structure of searchable content.\\n2. Uplo ad cont ent using the \"push\" model  to push JSON documents from any\\nsource, or use the \"pull\" model (indexers)  if your source data is of a supported\\ntype.\\n3. Quer y an index  using Search explorer  in the portal, REST API , .NET SDK , or another\\nSDK.How to get started\\n\\uea80 Tip\\nFor help with complex or custom solutions, contact a p artner with deep expertise\\nin Cognitive Search technology.', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.012048192322254181, '@search.rerankerScore': 1.3682597875595093, '@search.captions': [{'text': '1. On GitHub, fork the sample repository . Complete the fork process in your web browser with your GitHub account. This tutorial uses your fork as part of the deployment to an Azure S tatic W eb App. 2. At a bash terminal, download the sample application to your local computer.', 'highlights': '1. On GitHub, fork the<em> sample</em> repository . Complete the<em> fork</em> process in your web browser with your GitHub account. This tutorial uses<em> your fork</em> as part of the deployment to an Azure S tatic W eb App. 2. At a bash terminal, download<em> the sample</em> application to<em> your</em> local computer.'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjI0Ng==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_246', 'chunk': \"The sample  includes the following:\\nApp Purpose GitHub  \\nReposit ory \\nLocation\\nClient React app (presentation layer) to display books, with\\nsearch. It calls the Azure Function app./search-website-functions-\\nv4/client\\nServer Azure Function app (business layer) - calls the Azure\\nCognitive Search API using Python SDK/search-website-functions-\\nv4/api\\nBulk\\ninsertPython file to create the index and add documents to it. /search-website-functions-\\nv4/bulk-upload\\nInstall the following for your local development environment.\\nPython 3.9\\nGit\\nVisual S tudio Code  and the following extensions\\nAzure S tatic W eb App\\nOptional:\\nThis tutorial doesn't run the Azure Function API locally but if you intend to run it\\nlocally, you need to install azure-functions-core-tools .\\nForking the sample repository is critical to be able to deploy the static web app. The\\nweb apps determine the build actions and deployment content based on your own\\nGitHub fork location. Code execution in the S tatic W eb App is remote, with Azure static\\nweb apps reading from the code in your forked sample.\\n1. On GitHub, fork the sample repository .\\nComplete the fork process in your web browser with your GitHub account. This\\ntutorial uses your fork as part of the deployment to an Azure S tatic W eb App.\\n2. At a bash terminal, download the sample application to your local computer.\\nReplace YOUR-GITHUB-ALIAS with your GitHub alias.How is the sample organized?\\nSet up your development environment\\nFork and clone the search sample with git\\n\", 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.028991596773266792, '@search.rerankerScore': 1.3345587253570557, '@search.captions': [{'text': \"7. In the last example, we'll use the autocomplete function. Autocomplete is typically used in a search box to provide potential matches as the user types into the search box. When the index was created, a suggester named sg was also created as part of the request.\", 'highlights': \"7. In the last<em> example,</em> we'll use the<em> autocomplete</em> function.<em> Autocomplete is</em> typically used in a search box to provide potential matches as the user types into the search box. When the index was created, a suggester named sg was also created as part of the request.\"}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjE1MA==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_150', 'chunk': '7. In the last example, we\\'ll use the autocomplete function. Autocomplete is typically\\nused in a search box to provide potential matches as the user types into the search\\nbox.\\nWhen the index was created, a suggester named sg was also created as part of the\\nrequest. A suggester definition specifies which fields can be used to find potential\\nmatches to suggester requests. In this example, those fields are \\'T ags\\',\\n\\'Address/City\\', \\'Address/Country\\'. T o simulate auto-complete, pass in the letters\\n\"sa\" as a partial string. The autocomplete method of SearchClient  sends back\\npotential term matches.\\nPython\\nWhen you\\'re working in your own subscription, it\\'s a good idea at the end of a project\\nto identify whether you still need the resources you created. R esources left running can\\ncost you money. Y ou can delete resources individually or delete the resource group to\\ndelete the entire set of resources.\\nYou can find and manage resources in the portal, using the All resour ces or Resour ce\\ngroups  link in the left-navigation pane.\\nIf you\\'re using a free service, remember that you\\'re limited to three indexes, indexers,\\nand data sources. Y ou can delete individual items in the portal to stay under the limit.\\nIn this Python quickstart, you worked through the fundamental workflow using the\\nazure.search.documents library from the Python SDK. Y ou performed tasks that created\\nan index, loaded it with documents, and ran queries. T o continue learning, try the\\nfollowing tutorial.print(\"Rating: {}\" .format(result[ \"Rating\" ])) \\nprint(\"Category: {}\" .format(result[ \"Category\" ])) \\nsearch_suggestion = \\'sa\\' \\nresults = search_client.autocomplete(search_text=search_suggestion,  \\nsuggester_name= \"sg\", mode=\\'twoTerms\\' ) \\nprint(\"Autocomplete for:\" , search_suggestion)  \\nfor result in results:  \\n    print (result[ \\'text\\']) \\nClean up\\nNext steps', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.016129031777381897, '@search.rerankerScore': 1.333792805671692, '@search.captions': [{'text': 'For example, if a tags field starts with a value of [\"budget\"] and you execute a merge with [\"economy\", \"pool\"], the final value of the tags field is [\"economy\", \"pool\"].', 'highlights': 'For example, if a<em> tags field</em> starts<em> with</em> a value of [\"budget\"] and you execute a merge with [\"economy\", \"pool\"], the final value<em> of the tags field is</em> [\"economy\", \"pool\"].'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjUzNg==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_536', 'chunk': 'Add, Update, or Delete Documents (REST API)\\nIndexDocumentsAction class (Azure SDK for .NET)  or IndexDocumentsBatch class\\nThere is currently no tool support for pushing data via the portal.\\nFor an introduction to the push APIs, see:\\nC# Tutorial: Optimize indexing with the push API\\nC# Quickstart: Create an Azure Cognitive Search index using .NET SDK\\nREST Quickstart: Create an Azure Cognitive Search index using P owerShell\\nPython Quickstart: Create an Azure Cognitive Search index using the Azure SDK for\\nPython\\nJavaScript Quickstart: Create an Azure Cognitive Search index using the Azure SDK\\nfor JavaScript\\nYou can control the type of indexing action on a per-document basis, specifying\\nwhether the document should be uploaded in full, merged with existing document\\ncontent, or deleted.\\nWhether you use the REST API or an SDK, the following document operations are\\nsupported for data import:\\nUplo ad, similar to an \"upsert\" where the document is inserted if it is new, and\\nupdated or replaced if it exists. If the document is missing values that the index\\nrequires, the document field\\'s value is set to null.\\nmerge updates a document that already exists, and fails a document that cannot\\nbe found. Merge replaces existing values. For this reason, be sure to check for\\ncollection fields that contain multiple values, such as fields of type\\nCollection(Edm.String). For example, if a tags field starts with a value of\\n[\"budget\"] and you execute a merge with [\"economy\", \"pool\"], the final value of\\nthe tags field is [\"economy\", \"pool\"]. It won\\'t be [\"budget\", \"economy\", \"pool\"].\\nmergeOrUplo ad behaves like merge if the document exists, and upload if the\\ndocument is new.\\ndelet e removes the entire document from the index. If you want to remove an\\nindividual field, use merge instead, setting the field in question to null.Indexing actions: upload, merge, mergeOrUpload, delete\\nPulling  data into an index', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.025218810886144638, '@search.rerankerScore': 1.3223038911819458, '@search.captions': [{'text': 'python        complexfield(name= \"address\" , fields=[               simplefield(name= \"streetaddress\" ,  type=searchfielddatatype.string),             simplefield(name= \"city\", type=searchfielddatatype.string),           ])       ]  cors_options = corsoptions(allowed_origins=[ \"*\"], max_age_in_seconds= 60)  scoring_profiles = []   index = …', 'highlights': '<em>python</em>        complexfield(name=<em> \"address\" </em>, fields=[               simplefield(name=<em> \"streetaddress\"</em> ,  type=searchfielddatatype.string),             simplefield(name= \"city\", type=searchfielddatatype.string),           ])       ]  cors_options = corsoptions(allowed_origins=[ \"*\"], max_age_in_seconds= 60)  scoring_profiles = []<em>   index</em> = …'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjE2Njg=', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_1668', 'chunk': 'You can Upload, Merge, MergeOrUpload, and Delete multiple documents from an index\\nin a single batched request. There are a few special rules for merging  to be aware of.\\nPython        ComplexField(name= \"address\" , fields=[  \\n            SimpleField(name= \"streetAddress\" , \\ntype=SearchFieldDataType.String),\\n            SimpleField(name= \"city\", type=SearchFieldDataType.String),  \\n        ])  \\n    ] \\ncors_options = CorsOptions(allowed_origins=[ \"*\"], max_age_in_seconds= 60) \\nscoring_profiles = []  \\nindex = SearchIndex(  \\n    name=name,  \\n    fields=fields,  \\n    scoring_profiles=scoring_profiles,  \\n    cors_options=cors_options)  \\nresult = client.create_index(index)  \\nAdding documents to your index\\nimport os \\nfrom azure.core.credentials import AzureKeyCredential  \\nfrom azure.search.documents import SearchClient  \\nindex_name = \"hotels\"  \\nendpoint = os.environ[ \"SEARCH_ENDPOINT\" ] \\nkey = os.environ[ \"SEARCH_API_KEY\" ] \\nDOCUMENT = {  \\n    \\'Category\\' : \\'Hotel\\', \\n    \\'hotelId\\' : \\'1000\\', \\n    \\'rating\\' : 4.0, \\n    \\'rooms\\': [], \\n    \\'hotelName\\' : \\'Azure Inn\\' , \\n} \\nsearch_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))  \\nresult = search_client.upload_documents(documents=[DOCUMENT])  \\nprint(\"Upload of new document succeeded: {}\" .format(result[ 0].succeeded))  \\nAuthenticate in a National Cloud', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.010526316240429878, '@search.rerankerScore': 1.313112735748291, '@search.captions': [{'text': \"This tutorial uses Python and the Search REST APIs  to create a data source, index, indexer, and skillset. The indexer retrieves sample data in a blob container that's specified in the data source object, and sends all enriched content to a search index. The skillset is attached to the indexer.\", 'highlights': \"This tutorial uses<em> Python</em> and the Search REST APIs  to create a<em> data source, index, indexer,</em> and skillset. The<em> indexer</em> retrieves<em> sample data</em> in a blob container that's specified in the<em> data source</em> object, and sends all enriched content to a<em> search index.</em> The skillset is attached to the<em> indexer.</em>\"}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjQwOQ==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_409', 'chunk': \"Tutorial: Use Python and AI to generate\\nsearchable content from Azure blobs\\nArticle •08/26/2022 •16 minutes to read\\nIf you have unstructured text or images in Azure Blob S torage, an AI enrichment pipeline\\ncan extract information and create new content for full-text search or knowledge mining\\nscenarios.\\nIn this Python tutorial, you'll learn how to:\\nIf you don't have an Azure subscription, open a free account  before you begin.\\nThis tutorial uses Python and the Search REST APIs  to create a data source, index,\\nindexer, and skillset.\\nThe indexer retrieves sample data in a blob container that's specified in the data source\\nobject, and sends all enriched content to a search index.\\nThe skillset is attached to the indexer. It uses built-in skills from Microsoft to find and\\nextract information. S teps in the pipeline include Optical Character R ecognition (OCR)\\non images, language detection on text, key phrase extraction, and entity recognition\\n(organizations). New information created by the pipeline is stored in new fields in an\\nindex. Once the index is populated, you can use the fields in queries, facets, and filters.\\nVisual S tudio Code  with the Python extension  and Python 3.7 or later\\nAzure S torage\\nAzure Cognitive SearchSet up a development environment＂\\nDefine a pipeline that uses OCR, language detection, and entity and key phrase\\nrecognition.＂\\nExecute the pipeline to invoke transformations, and to create and load a search\\nindex.＂\\nExplore results using full text search and a rich query syntax.＂\\nOverview\\nPrerequisites\\n７ Note\", 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.009523809887468815, '@search.rerankerScore': 1.3100489377975464, '@search.captions': [{'text': 'The Suggest API  takes a search term while a user is typing and suggests search terms such as book titles and authors across the documents in the search index, returning a small list of matches. The search suggester, sg, is defined in the schema file  used during bulk upload. Routing for the Suggest API is contained in the function.json  bindings.', 'highlights': 'The Suggest API  takes a<em> search</em> term while a user is typing and suggests<em> search</em> terms such as book titles and authors across the<em> documents in</em> the<em> search index,</em> returning a small list of matches. The<em> search</em> suggester, sg, is defined in the schema file  used during bulk upload. Routing for the Suggest API is contained in the function.json  bindings.'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjI3NQ==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_275', 'chunk': 'The Suggest API  takes a search term while a user is typing and suggests search terms\\nsuch as book titles and authors across the documents in the search index, returning a\\nsmall list of matches.\\nThe search suggester, sg, is defined in the schema file  used during bulk upload.\\nRouting for the Suggest API is contained in the function.json  bindings.\\nPython            <SearchBar  postSearchHandler ={postSearchHandler}  q={q}>\\n</SearchBar > \\n          </div> \\n          <Facets facets={facets}  filters={filters}  setFilters ={setFilters} >\\n</Facets> \\n        </div> \\n        {body}  \\n      </div> \\n    </main> \\n  ); \\n} \\nAzure Function: Sugg estions from the catalog\\nimport logging  \\nimport azure.functions as func \\nfrom azure.core.credentials import AzureKeyCredential  \\nfrom azure.search.documents import SearchClient  \\nfrom shared_code import azure_config  \\nimport json \\nenvironment_vars = azure_config()\\n# curl --header \"Content-Type: application/json\" \\\\  \\n#  --request POST \\\\  \\n#  --data \\'{\"q\":\"code\",\"top\":\"5\", \"suggester\":\"sg\"}\\' \\\\  \\n#  http://localhost:7071/api/Suggest  \\n# Set Azure Search endpoint and key  \\nservice_name = environment_vars[ \"search_service_name\" ] \\nendpoint = f\"https:// {service_name} .search.windows.net\"  \\nkey = environment_vars[ \"search_api_key\" ] \\n# Your index name  \\nindex_name = \"good-books\"  \\n# Create Azure SDK client  \\nsearch_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}, {'@search.score': 0.02819138392806053, '@search.rerankerScore': 1.3073681592941284, '@search.captions': [{'text': 'Demonstrates an end-to-end search app that includes a rich client plus components for hosting the app and handling search requests. tutorial-ai- enrichmentSource code for Tutorial: Use Python and AI to generate searchable content from Azure blobs .', 'highlights': 'Demonstrates an end-to-end<em> search</em> app that includes a rich client plus components<em> for</em> hosting the app and handling<em> search</em> requests. tutorial-ai- enrichmentSource code for Tutorial:<em> Use Python and</em> AI to generate searchable content from Azure blobs .'}], 'id': 'QXp1cmVfQ29nbml0aXZlX1NlYXJjaF9Eb2N1bWVudGF0aW9uLnBkZjQ4Ng==', 'title': 'Azure_Cognitive_Search_Documentation.pdf_page_486', 'chunk': 'Additional r esour ces\\n\\uef14DocumentationSamples Descr iption\\nSynonyms Demonstrates how to create, update, get, list, and delete synonym\\nmaps .\\nLoad documents Demonstrates how to upload or merge documents into an index in a\\ndata import  operation.\\nSimple query Demonstrates how to set up a basic query .\\nFilter query Demonstrates setting up a filter expression .\\nFacet query Demonstrates working with facets .\\nCode samples from the Cognitive Search team demonstrate features and workflows.\\nMany of these samples are referenced in tutorials, quickstarts, and how-to articles. Y ou\\ncan find these samples in Azur e-Samples/azur e-sear ch-py thon-samples  on GitHub.\\nSamples Article\\nquickstart Source code for Quickstart: Create a search index in Python . This article covers the\\nbasic workflow for creating, loading, and querying a search index using sample\\ndata.\\nsearch-\\nwebsiteSource code for Tutorial: Add search to web apps . Demonstrates an end-to-end\\nsearch app that includes a rich client plus components for hosting the app and\\nhandling search requests.\\ntutorial-ai-\\nenrichmentSource code for Tutorial: Use Python and AI to generate searchable content from\\nAzure blobs . This article shows how to create a blob indexer with a cognitive\\nskillset, where the skillset creates and transforms raw content to make it\\nsearchable or consumable.\\nDoc samples\\n\\uea80 Tip\\nTry the Samples br owser  to search for Microsoft code samples in GitHub, filtered\\nby product, service, and language.', 'name': 'Azure_Cognitive_Search_Documentation.pdf', 'location': 'https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf'}]}\n"
          ]
        }
      ],
      "source": [
        "vector_indexes = [book_index_name]\n",
        "\n",
        "ordered_results = get_search_results(QUESTION, vector_indexes, \n",
        "                                        k=10,\n",
        "                                        reranker_threshold=1,\n",
        "                                        vector_search=True, \n",
        "                                        similarity_k=10,\n",
        "                                        query_vector = embedder.embed_query(QUESTION)\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd2f3f2-2d66-4bd4-b90b-d30970b71af4",
      "metadata": {},
      "source": [
        "**Note**: that we are picking a larger k=10 since these chunks are NOT of 5000 chars each like prior notebooks, but instead each page is a chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "410ff796-dab1-4817-a3a5-82eeff6c0c57",
      "metadata": {},
      "outputs": [],
      "source": [
        "COMPLETION_TOKENS = 1000\n",
        "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "744aba20-b3fd-4286-8d58-2ddfccc77734",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 10\n"
          ]
        }
      ],
      "source": [
        "top_docs = []\n",
        "for key,value in ordered_results.items():\n",
        "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
        "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
        "        \n",
        "print(\"Number of chunks:\",len(top_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "db1c4d56-8c2d-47d6-8717-810f156f1c0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System prompt token count: 1669\n",
            "Max Completion Token count: 1000\n",
            "Combined docs (context) token count: 3595\n",
            "--------\n",
            "Requested token count: 6264\n",
            "Token limit for gpt-4-32k : 32768\n",
            "Chain Type selected: stuff\n"
          ]
        }
      ],
      "source": [
        "# Calculate number of tokens of our docs\n",
        "if(len(top_docs)>0):\n",
        "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
        "    prompt_tokens = num_tokens_from_string(COMBINE_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
        "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
        "    \n",
        "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
        "    \n",
        "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
        "    \n",
        "    print(\"System prompt token count:\",prompt_tokens)\n",
        "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
        "    print(\"Combined docs (context) token count:\",context_tokens)\n",
        "    print(\"--------\")\n",
        "    print(\"Requested token count:\",requested_tokens)\n",
        "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
        "    print(\"Chain Type selected:\", chain_type)\n",
        "        \n",
        "else:\n",
        "    print(\"NO RESULTS FROM AZURE SEARCH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "62cf3a3f-2b4d-4806-8b92-eb982c52b0cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "if chain_type == \"stuff\":\n",
        "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
        "                                       prompt=COMBINE_PROMPT)\n",
        "elif chain_type == \"map_reduce\":\n",
        "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
        "                                       question_prompt=COMBINE_QUESTION_PROMPT,\n",
        "                                       combine_prompt=COMBINE_PROMPT,\n",
        "                                       return_intermediate_steps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3b412c56-650f-4ca4-a868-9954f83679fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 31.2 ms\n",
            "Wall time: 20.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Try with other language as well\n",
        "response = chain({\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"English\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "aa62cf98",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To push documents with vectors to an index using the Python SDK, you can follow these steps:\n",
              "\n",
              "1. Create an index. You can specify fields and other options such as cors_options and scoring_profiles. Here is an example of creating an index:\n",
              "\n",
              "```python\n",
              "fields = [  \n",
              "    SearchableField(name= \"Country\", type=SearchFieldDataType.String, facetable= True, filterable= True, sortable= True), \n",
              "]\n",
              "cors_options = CorsOptions(allowed_origins=[ \"*\"], max_age_in_seconds= 60) \n",
              "scoring_profiles = []  \n",
              "index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)  \n",
              "try: \n",
              "    result = admin_client.create_index(index)  \n",
              "    print ('Index', result.name, 'created' ) \n",
              "except Exception as ex:\n",
              "    print (ex) \n",
              "```\n",
              "\n",
              "2. Load documents. You can create a collection of documents and specify an upload action for each document. Here is an example of loading documents:\n",
              "\n",
              "```python\n",
              "documents = [  \n",
              "    { \n",
              "    \"@search.action\" : \"upload\" , \n",
              "    \"HotelId\" : \"1\", \n",
              "    \"HotelName\" : \"Secret Point Motel\" , \n",
              "    ...\n",
              "    } \n",
              "]\n",
              "try: \n",
              "    result = search_client.upload_documents(documents=documents)  \n",
              "    print( \"Upload of new document succeeded: {}\".format(result[ 0].succeeded))  \n",
              "except Exception as ex:\n",
              "    print (ex.message)  \n",
              "```\n",
              "\n",
              "3. Query the index. You can use the search method of the search.client class to query the index. Here is an example of querying an index:\n",
              "\n",
              "```python\n",
              "results = search_client.search(search_text= \"*\", include_total_count= True) \n",
              "print ('Total Documents Matching Query:' , results.get_count())\n",
              "```\n",
              "\n",
              "All these examples are extracted from the Azure Cognitive Search Documentation<sup><a href=\"https://datasetsgptsmartsearch.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation.pdf?sv=2022-11-02&ss=b&srt=sco&sp=rl&se=2026-01-03T02:11:44Z&st=2024-01-02T18:11:44Z&spr=https&sig=ngrEqvqBVaxyuSYqgPVeF%2B9c0fXLs94v3ASgwg7LDBs%3D\" target=\"_blank\">[1]</a></sup>."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['output_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3941796c-7655-4888-a358-8a62e380bd7e",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "In this notebook we learned how to deal with complex and large Documents and make them available for Q&A over them using [Hybrid Search](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector#hybrid-search) (text + vector search).\n",
        "\n",
        "We also learned the power of Azure Document Inteligence API and why it is recommended for production scenarios where manual Document parsing (instead of Azure Search Indexer Document Cracking) is necessary.\n",
        "\n",
        "Using Azure Cognitive Search with its Vector capabilities and hybrid search features eliminates the need for other vector databases such as Weaviate, Qdrant, Milvus, Pinecone, and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d9a7d1-f029-416b-8eb2-00a8afb9151d",
      "metadata": {},
      "source": [
        "# NEXT\n",
        "So far we have learned how to use OpenAI vectors and completion APIs in order to get an excelent answer from our documents stored in Azure Cognitive Search. This is the backbone for a GPT Smart Search Engine.\n",
        "\n",
        "However, we are missing something: **How to have a conversation with this engine?**\n",
        "\n",
        "On the next Notebook, we are going to understand the concept of **memory**. This is necessary in order to have a chatbot that can establish a conversation with the user. Without memory, there is no real conversation."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
